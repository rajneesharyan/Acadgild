{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Problem Statement: Machine Learning 1\n",
    "1. What are the three stages to build the hypotheses or model in machine learning?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A machine learning model which is can be a mathematical representation of a real-world process. The learning algorithm finds patterns in the training data which is that the input parameters correspond to the target. The output of the training process is a machine learning model which is can use to make predictions.\n",
    "\n",
    "The three stages of building the hypotheses or model in machine learning:\n",
    "\n",
    "     Data set preparation which is the foundation for any machine learning.\n",
    "     The project implementation is complex and involves data collection selection.\n",
    "     The pre-processing and transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the standard approach to supervised learning?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is Training set and Test set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set is the data set on which your model is built.\n",
    "Training set is usually manually written and your model follows exactly the same rules \n",
    "and definitions given in the training set.\n",
    "\n",
    "\n",
    "Test set is the data set on which you apply your model and see if it is working correctly and yielding expected\n",
    "and desired results or not. Test set is like a test to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general principle of an ensemble method is to combine the predictions of several models built \n",
    "with a given learning algorithm in order to improve robustness over a single model.\n",
    "bagging is a method in ensemble for improving unstable estimation or classification schemes.\n",
    "\n",
    "\n",
    "Ensemble model combines multiple ‘individual’ (diverse) models together and delivers superior prediction power.\n",
    "An ensemble is a supervised learning technique for combining multiple weak learners/ models to produce a strong learner.\n",
    "Ensemble model works better, when we ensemble models with low correlation.\n",
    "\n",
    "\n",
    "Ensemble learning is a machine learning paradigm where multiple models (often called “weak learners”) are trained to solve\n",
    "the same problem and combined to get better results.\n",
    "The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.\n",
    "\n",
    "Single weak learner\n",
    "In machine learning, no matter if we are facing a classification or a regression problem, the choice of the model is extremely important to have any chance to obtain good results. This choice can depend on many variables of the problem: quantity of data, dimensionality of the space, distribution hypothesis…\n",
    "\n",
    "A low bias and a low variance, although they most often vary in opposite directions, are the two most fundamental features \n",
    "expected for a model. Indeed, to be able to “solve” a problem, we want our model to have enough degrees of freedom to resolve \n",
    "the underlying complexity of the data we are working with,\n",
    "but we also want it to have not too much degrees of freedom to avoid high variance and be more robust. \n",
    "This is the well known bias-variance tradeoff.\n",
    "\n",
    "\n",
    "\n",
    "In ensemble learning theory, we call weak learners (or base models) models that can be used as building blocks for\n",
    "designing more complex models by combining several of them. Most of the time,\n",
    "these basics models perform not so well by themselves either because they have a high bias (low degree of freedom models,\n",
    " for example) or because they have too much variance to be robust (high degree of freedom models, for example). \n",
    "Then, the idea of ensemble methods is to try reducing bias and/or variance of such weak learners by combining several \n",
    "of them together in order to create a strong learner (or ensemble model) that achieves better performances.\n",
    "\n",
    "Combine weak learners\n",
    "In order to set up an ensemble learning method, we first need to select our base models to be aggregated. Most of the time (including in the well known bagging and boosting methods) a single base learning algorithm is used so that we have homogeneous weak learners that are trained in different ways. The ensemble model we obtain is then said to be “homogeneous”. However, there also exist some methods that use different type of base learning algorithms: some heterogeneous weak learners are then combined into an “heterogeneous ensembles model”.\n",
    "\n",
    "One important point is that our choice of weak learners should be coherent with the way we aggregate these models. If we choose base models with low bias but high variance, it should be with an aggregating method that tends to reduce variance whereas if we choose base models with low variance but high bias, it should be with an aggregating method that tends to reduce bias.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bagging (Bootstrap Aggregating)  is an ensemble method. \n",
    "First, we create random samples of the training data set (sub sets of training data set). \n",
    "Then, we build a classifier for each sample. \n",
    "Finally, results of these multiple classifiers are combined using average or majority voting.\n",
    "Bagging helps to reduce the variance error.\n",
    "\n",
    "Bagging,that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following\n",
    "some kind of deterministic averaging process\n",
    "\n",
    "\n",
    "Boosting provides sequential learning of the predictors. \n",
    "The first predictor is learned on the whole data set, while the following are learnt on the training set \n",
    "based on the performance of the previous one.\n",
    "It starts by classifying original data set and giving equal weights to each observation. \n",
    "If classes are predicted incorrectly using the first learner, then it gives higher weight to the missed classified observation.\n",
    "Being an iterative process, it continues to add classifier learner until a limit is reached in the number of models or accuracy.\n",
    "Boosting has shown better predictive accuracy than bagging, but it also tends to over-fit the training data as well. \n",
    "\n",
    "Boosting,that often considers homogeneous weak learners,\n",
    "learns them sequentially in a very adaptative way (a base model depends on the previous ones) and \n",
    "combines them following a deterministic strategy\n",
    "\n",
    "There are two major benefits of Ensemble models:\n",
    "\n",
    "Better prediction\n",
    "More stable model\n",
    "\n",
    "\n",
    "it also has stacking,\n",
    "that often considers heterogeneous weak learners,\n",
    "learns them in parallel and combines them by training a meta-model to output \n",
    "a prediction based on the different weak models predictions\n",
    "\n",
    "\n",
    "\n",
    "OR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nsembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:\n",
    "\n",
    "Bagging attempts to reduce the chance overfitting complex models.\n",
    "\n",
    "It trains a large number of \"strong\" learners in parallel.\n",
    "A strong learner is a model that's relatively unconstrained.\n",
    "Bagging then combines all the strong learners together in order to \"smooth out\" their predictions.\n",
    "Boosting attempts to improve the predictive flexibility of simple models.\n",
    "\n",
    "It trains a large number of \"weak\" learners in sequence.\n",
    "A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).\n",
    "Each one in the sequence focuses on learning from the mistakes of the one before it.\n",
    "Boosting then combines all the weak learners into a single strong learner.\n",
    "While bagging and boosting are both ensemble methods, they approach the problem from opposite directions.\n",
    "\n",
    "Bagging uses complex base models and tries to \"smooth out\" their predictions, while boosting uses simple base models and tries to \"boost\" their aggregate complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How can you avoid overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting :  model doesn’t generalize well from our training data to unseen data.\n",
    "Detecting overfitting is useful, but it doesn’t solve the problem. Fortunately, you have several options to try.\n",
    "\n",
    "Here are a few of the most popular solutions for overfitting:\n",
    "\n",
    "Cross-validation\n",
    "    Cross-validation is a powerful preventative measure against overfitting.\n",
    "The idea is clever: Use your initial training data to generate multiple mini train-test splits.\n",
    "Use these splits to tune your model.\n",
    "\n",
    "In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the “holdout fold”).\n",
    "\n",
    "K-Fold Cross-Validation\n",
    "K-Fold Cross-Validation\n",
    "\n",
    "Cross-validation allows you to tune hyperparameters with only your original training set. \n",
    "This allows you to keep your test set as a truly unseen dataset for selecting your final model.\n",
    "\n",
    "\n",
    "Train with more data\n",
    "    It won’t work every time, but training with more data can help algorithms detect the signal better.\n",
    "   If we just add more noisy data, this technique won’t help. \n",
    "    That’s why you should always ensure your data is clean and relevant.\n",
    "\n",
    "\n",
    "Remove features\n",
    "Some algorithms have built-in feature selection.\n",
    "\n",
    "For those that don’t, you can manually improve their generalizability by removing irrelevant input features.\n",
    "\n",
    "An interesting way to do so is to tell a story about how each feature fits into the model.\n",
    "This is like the data scientist's spin on software engineer’s rubber duck debugging technique,\n",
    "where they debug their code by explaining it, line-by-line, to a rubber duck.\n",
    "\n",
    "If anything doesn't make sense, or if it’s hard to justify certain features, this is a good way to identify them.\n",
    "In addition, there are several feature selection heuristics you can use for a good starting point.\n",
    "\n",
    "Early stopping\n",
    "When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs.\n",
    "Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.\n",
    "\n",
    "Early stopping refers stopping the training process before the learner passes that point.\n",
    "\n",
    "Regularization\n",
    "Regularization refers to a broad range of techniques for artificially forcing your model to be simpler.\n",
    "\n",
    "The method will depend on the type of learner you’re using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n",
    "\n",
    "Oftentimes, the regularization method is a hyperparameter as well, which means it can be tuned through cross-validation.\n",
    "\n",
    "We have a more detailed discussion here on algorithms and regularization methods.\n",
    "\n",
    "Ensembling\n",
    "Ensembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:\n",
    "\n",
    "Bagging attempts to reduce the chance overfitting complex models.\n",
    "\n",
    "It trains a large number of \"strong\" learners in parallel.\n",
    "A strong learner is a model that's relatively unconstrained.\n",
    "Bagging then combines all the strong learners together in order to \"smooth out\" their predictions.\n",
    "Boosting attempts to improve the predictive flexibility of simple models.\n",
    "\n",
    "It trains a large number of \"weak\" learners in sequence.\n",
    "A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).\n",
    "Each one in the sequence focuses on learning from the mistakes of the one before it.\n",
    "Boosting then combines all the weak learners into a single strong learner.\n",
    "While bagging and boosting are both ensemble methods, they approach the problem from opposite directions.\n",
    "\n",
    "Bagging uses complex base models and tries to \"smooth out\" their predictions, while boosting uses simple base models and tries to \"boost\" their aggregate complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Problem Statement: Machine Learning 2\n",
    "Build the linear regression model using scikit learn in boston data to predict\n",
    "'Price' based on other dependent variable.\n",
    "Here is the code to load the data:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "bos = pd.DataFrame(boston.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
